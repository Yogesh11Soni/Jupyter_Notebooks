{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement learning is all about making decisions sequentially. In simple words, we can say that the output depends on the state of the current input and the next input depends on the output of the previous input\n",
    "RL is a type of machine learning, in which an agent explores an environment to learn how to perform desired tasks by taking actions with good outcomes and avoiding actions with bad outcomes \n",
    "A Reinforcement learning model will learn from its experience and over tme will be able to identify which action leads to the best reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent and Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reinforcement learning problem is meant to be a straightforward framing of the problem of learning from interaction to achieve a goal. The learner and decision-maker is called the agent. The thing it interacts with, comprising everything outside the agent, is called the environment. These interact continually, the agent selecting actions and the environment responding to those actions and presenting new situations to the agent. The environment also gives rise to rewards, special numerical values that the agent tries to maximize over time. A complete specification of an environment defines a task, one instance of the reinforcement learning problem.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of RL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the things very simple, lets create a dummy environment that gives the agent some random rewards everytime, regardless tof the agent's action\n",
    "Though this is not of any practical usage, it allows us to focus on implementaion of environment and agent classes.\n",
    "\n",
    "Our environment class should be capable of handling actions received from the agent. This is done by action method, which checks the number of steps left and returns a random reward, by  ignoring the agent's actions.\n",
    "\n",
    "__init__ constructor is called to set the number of episodes for the event. get_observation() method is supposed to return the current environment's observation to the agent, but in this case returns a zero vector.\n",
    "\n",
    "Other methods are mostly self explanatory. get_actions retirns 0 or 1 to corresponding to two available actions. is_done checks the end of episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "from typing import List\n",
    "\n",
    "class SampleEnvironment:\n",
    "    def __init__(self):\n",
    "        self.steps_left = 20\n",
    "        \n",
    "    def get_observation(self) -> List[float]:\n",
    "        return [0.0, 0.0, 0.0]\n",
    "    \n",
    "    def get_actions(self) -> List[int]:\n",
    "        return[0, 1]\n",
    "    \n",
    "    def is_done(self) -> bool:\n",
    "        return self.steps_left == 0\n",
    "\n",
    "    def action(self, action: int) -> float:\n",
    "        if self.is_done():\n",
    "            raise Exception(\"Game is Over\")\n",
    "        self.steps_left -= 1\n",
    "        return random.random()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.total_reward = 0.0\n",
    "        \n",
    "    def steps(self, env: SampleEnvironment):\n",
    "        current_obs = env.get_observation()\n",
    "        print(\"Observation {}\".format(current_obs))\n",
    "        actions = env.get_actions()\n",
    "        print(actions)\n",
    "        reward = env.action(random.choice(actions))\n",
    "        self.total_reward += reward\n",
    "        print(\"Total Reward {}\".format(self.total_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps 1\n",
      "Observation [0.0, 0.0, 0.0]\n",
      "[0, 1]\n",
      "Total Reward 0.8996860915269408\n",
      "Steps 2\n",
      "Observation [0.0, 0.0, 0.0]\n",
      "[0, 1]\n",
      "Total Reward 1.6590099392408124\n",
      "Steps 3\n",
      "Observation [0.0, 0.0, 0.0]\n",
      "[0, 1]\n",
      "Total Reward 2.2245356638652853\n",
      "Steps 4\n",
      "Observation [0.0, 0.0, 0.0]\n",
      "[0, 1]\n",
      "Total Reward 2.617391324255101\n",
      "Steps 5\n",
      "Observation [0.0, 0.0, 0.0]\n",
      "[0, 1]\n",
      "Total Reward 3.336619009567398\n",
      "Steps 6\n",
      "Observation [0.0, 0.0, 0.0]\n",
      "[0, 1]\n",
      "Total Reward 3.7106371643204947\n",
      "Steps 7\n",
      "Observation [0.0, 0.0, 0.0]\n",
      "[0, 1]\n",
      "Total Reward 4.1568019415006665\n",
      "Steps 8\n",
      "Observation [0.0, 0.0, 0.0]\n",
      "[0, 1]\n",
      "Total Reward 5.1315200909112155\n",
      "Steps 9\n",
      "Observation [0.0, 0.0, 0.0]\n",
      "[0, 1]\n",
      "Total Reward 6.080286552007688\n",
      "Steps 10\n",
      "Observation [0.0, 0.0, 0.0]\n",
      "[0, 1]\n",
      "Total Reward 6.953686376050603\n",
      "Steps 11\n",
      "Observation [0.0, 0.0, 0.0]\n",
      "[0, 1]\n",
      "Total Reward 7.867973697183889\n",
      "Steps 12\n",
      "Observation [0.0, 0.0, 0.0]\n",
      "[0, 1]\n",
      "Total Reward 7.890719817843685\n",
      "Steps 13\n",
      "Observation [0.0, 0.0, 0.0]\n",
      "[0, 1]\n",
      "Total Reward 8.067577627931858\n",
      "Steps 14\n",
      "Observation [0.0, 0.0, 0.0]\n",
      "[0, 1]\n",
      "Total Reward 8.799056055385757\n",
      "Steps 15\n",
      "Observation [0.0, 0.0, 0.0]\n",
      "[0, 1]\n",
      "Total Reward 9.030547907541008\n",
      "Steps 16\n",
      "Observation [0.0, 0.0, 0.0]\n",
      "[0, 1]\n",
      "Total Reward 9.970826687597658\n",
      "Steps 17\n",
      "Observation [0.0, 0.0, 0.0]\n",
      "[0, 1]\n",
      "Total Reward 10.076208088979334\n",
      "Steps 18\n",
      "Observation [0.0, 0.0, 0.0]\n",
      "[0, 1]\n",
      "Total Reward 10.824154242644259\n",
      "Steps 19\n",
      "Observation [0.0, 0.0, 0.0]\n",
      "[0, 1]\n",
      "Total Reward 11.704498932127187\n",
      "Steps 20\n",
      "Observation [0.0, 0.0, 0.0]\n",
      "[0, 1]\n",
      "Total Reward 12.076909384994023\n",
      "Total rewars got : 12.0769\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = SampleEnvironment()\n",
    "    agent = Agent()\n",
    "    i=0\n",
    "\n",
    "    while not env.is_done():\n",
    "        i=i+1\n",
    "        print(\"Steps {}\".format(i))\n",
    "        agent.steps(env)\n",
    "        \n",
    "    print(\"Total rewars got : %.4f\" % agent.total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
