{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d1f48f5-fc0c-4d94-99ea-daf16b0ee09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'if', 'or', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some']\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import sent_tokenize, regexp_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import re \n",
    "\n",
    "words = stopwords.words('english')[:115]\n",
    "words.remove('but')\n",
    "words.remove('because')\n",
    "\n",
    "print(words)\n",
    "class FindSentiment:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.sent_analyser  = SentimentIntensityAnalyzer()\n",
    "\n",
    "        self.connecting_words = ['and', 'but', 'or', 'As a result', 'As a consequence', 'Thus', 'Consequently', 'Hence', 'For this reason',\n",
    "            'Due to', 'In addition', 'Whereas', 'Before', 'Moreover', 'Alternatively', 'Subsequently', 'Not only but also', 'Conversely', \n",
    "            'Above all', 'in addition to this', 'Even so', 'Last but not least','Apart from this', 'Differing from', 'First and foremost',\n",
    "            'Illustration', 'Comparison', 'Summary', 'Similarly', 'In conclusion', 'Such as','Equally', 'To summarise', 'Including', 'Likewise',\n",
    "            'Altogether', 'Namely', 'Just as', 'In short', 'in this case', 'Just like', 'To sum up','Proof of this', 'Similar to', 'In summary',\n",
    "            'Same as', 'Briefly', 'To demonstrate','To conclude', 'In the same way', 'Reason','Because of', 'With this in mind in fact',\n",
    "            ]\n",
    "\n",
    "        self.stopwords  = words\n",
    "        self.tokenizer = RegexpTokenizer(\"[\\w]+\")\n",
    "        self.sentences_sentiments = []\n",
    "        \n",
    "        self.replace = { \n",
    "            \"aren't\" : \"are not\",  \"can't\" : \"cannot\", \"cant\" : \"cannot\", \"couldn't\" : \"could not\", \"didn't\" : \"did not\",\n",
    "            \"doesn't\" : \"does not\", \"don't\" : \"do not\", \"hadn't\" : \"had not\", \"hasn't\" : \"has not\", \"haven't\" : \"have not\",\n",
    "            \"he'd\" : \"he would\", \"he'll\" : \"he will\", \"he's\" : \"he is\", \"i'd\" : \"I would\", \"i'd\" : \"I had\", \"i'll\" : \"I will\",\n",
    "            \"i'm\" : \"I am\", \"im\" : \"I am\", \"isn't\" : \"is not\", \"it's\" : \"it is\", \"it'll\":\"it will\", \"i've\" : \"I have\",\n",
    "            \"let's\" : \"let us\", \"mightn't\" : \"might not\", \"mustn't\" : \"must not\", \"shan't\" : \"shall not\", \"she'd\" : \"she would\",\n",
    "            \"she'll\" : \"she will\", \"she's\" : \"she is\", \"shouldn't\" : \"should not\", \"that's\" : \"that is\", \"there's\" : \"there is\",\n",
    "            \"they'd\" : \"they would\", \"they'll\" : \"they will\", \"they're\" : \"they are\", \"they've\" : \"they have\", \"we'd\" : \"we would\",\n",
    "            \"we're\" : \"we are\", \"weren't\" : \"were not\", \"we've\" : \"we have\",\"what'll\" : \"what will\", \"what're\" : \"what are\", \n",
    "            \"what's\" : \"what is\", \"what've\" : \"what have\", \"where's\" : \"where is\", \"who'd\" : \"who would\", \"who'll\" : \"who will\",\n",
    "            \"who're\" : \"who are\", \"who's\" : \"who is\",\"who've\" : \"who have\", \"won't\" : \"will not\", \"wouldn't\" : \"would not\",\n",
    "            \"you'd\" : \"you would\", \"you'll\" : \"you will\",\"you're\" : \"you are\", \"you've\" : \"you have\",\n",
    "            \"'re\": \" are\", \"wasn't\": \"was not\", \"we'll\":\" will\", \"didn't\": \"did not\"\n",
    "        }\n",
    "    \n",
    "    def preprocess_sentence(self, sentence):\n",
    "        # print('sentence', sentence)\n",
    "        words = self.tokenizer.tokenize(sentence)\n",
    "        # print(words)\n",
    "\n",
    "        l = []\n",
    "        for word in words:\n",
    "            if re.search('[\\w\\']+', word):\n",
    "                if word not in self.stopwords:\n",
    "                # temp = re.sub(\"[^\\w]\", '' , word)\n",
    "                # if temp != \"\":\n",
    "                    l.append(word)\n",
    "\n",
    "\n",
    "        # print(l , '*'*2,)\n",
    "        return ' '.join(l)\n",
    "\n",
    "    def find_sentiment_per_line(self, paragraph):\n",
    "        sentences = sent_tokenize(paragraph)\n",
    "        # print(sentences)\n",
    "        for sentence in sentences:\n",
    "            s2 = \"\"\n",
    "\n",
    "            sentence_break = False\n",
    "            for connecting_word in self.connecting_words:\n",
    "                if re.search(f'\\s{connecting_word}\\s', sentence.lower()):\n",
    "                    sentence_break = True\n",
    "                    s1, s2 = sentence.split(connecting_word,1)\n",
    "                    # print(s1,s2)\n",
    "                    # sentence = s2\n",
    "                    # print(s1, \"*\", s2, \"per line\")\n",
    "\n",
    "                    s1 = self.preprocess_sentence(s1)\n",
    "                    # s2 = self.preprocess_sentence(s2)\n",
    "                    # if count == 0: \n",
    "                    neg, neu , pos, compound = self.sent_analyser.polarity_scores(s1).values()\n",
    "                    self.sentences_sentiments.append((neg,pos,compound,s1))\n",
    "                    sentence = s2 \n",
    "      \n",
    "            s2 = self.preprocess_sentence(s2)\n",
    "\n",
    "            neg1, neu , pos1, compound =  self.sent_analyser.polarity_scores(s2).values()\n",
    "            self.sentences_sentiments.append((neg1,pos1,compound,s2))\n",
    "\n",
    "\n",
    "\n",
    "    def find(self, paragraph):\n",
    "        neg, neu , pos, compound = self.sent_analyser.polarity_scores(paragraph).values()\n",
    "        print(neg, neu , pos, compound)\n",
    "                \n",
    "        if neg >=.1:\n",
    "            self.find_sentiment_per_line(paragraph)\n",
    "            self.sentences_sentiments.sort()\n",
    "            self.sentences_sentiments.reverse()\n",
    "            # print(self.sentences_sentiments)\n",
    "            # self.sentences_sentiments[0][1]\n",
    "\n",
    "            if self.sentences_sentiments[0][0] >.5:\n",
    "                return 'negative'\n",
    "\n",
    "        else:\n",
    "            if compound >= 0.05 :\n",
    "                return \"Positive\"\n",
    "                ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'if', 'or', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some']  \n",
    "            elif compound <= - 0.05 :\n",
    "                return \"Negative\"\n",
    "\n",
    "            else :\n",
    "                return \"Neutral\"\n",
    "            \n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb99d1e2-e057-4e97-8b94-cd7bd72bb242",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 1.0 0.0 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Neutral'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj = FindSentiment()\n",
    "obj.find(\"i am robo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99277d41-e492-4d42-b11c-87e739ac85f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bad', 'boy']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "\t# split into tokens by white space\n",
    "\ttokens = doc.split()\n",
    "\t# remove punctuation from each token\n",
    "\ttable = str.maketrans('', '', string.punctuation)\n",
    "\ttokens = [w.translate(table) for w in tokens]\n",
    "\t# remove remaining tokens that are not alphabetic\n",
    "\ttokens = [word for word in tokens if word.isalpha()]\n",
    "\t# filter out stop words\n",
    "\tstop_words = set(stopwords.words('english'))\n",
    "\ttokens = [w for w in tokens if not w in stop_words]\n",
    "\t# filter out short tokens\n",
    "\ttokens = [word for word in tokens if len(word) > 1]\n",
    "\treturn tokens\n",
    "\n",
    "# load the document\n",
    "filename = 'text.txt'\n",
    "text = load_doc(filename)\n",
    "tokens = clean_doc(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cec3f3-c988-4b3e-a8af-c52115f4beef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
